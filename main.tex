%%%%%%%% ICML 2026 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{longtable}
\usepackage{siunitx}
\usepackage{algorithm}
\usepackage{algorithmic}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% For preprint, use
% \usepackage[preprint]{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SPOCK: Scalable and Structure-Preserving Optimal Transport based Clustering with Kernel-density-estimation for Imperfect Multi-View Data}

\begin{document}

\twocolumn[
  \icmltitle{SPOCK: Scalable and Structure-Preserving Optimal Transport based Clustering with Kernel-density-estimation for Imperfect Multi-View Data}

  % It is OKAY to include author information, even for blind submissions: the
  % style file will automatically remove it for you unless you've provided
  % the [accepted] option to the icml2026 package.

  % List of affiliations: The first argument should be a (short) identifier you
  % will use later to specify author affiliations Academic affiliations
  % should list Department, University, City, Region, Country Industry
  % affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order. Ideally, you
  % should not use this facility. Affiliations will be numbered in order of
  % appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Firstname1 Lastname1}{equal,yyy}
    \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
    \icmlauthor{Firstname3 Lastname3}{comp}
    \icmlauthor{Firstname4 Lastname4}{sch}
    \icmlauthor{Firstname5 Lastname5}{yyy}
    \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
    \icmlauthor{Firstname7 Lastname7}{comp}
    %\icmlauthor{}{sch}
    \icmlauthor{Firstname8 Lastname8}{sch}
    \icmlauthor{Firstname8 Lastname8}{yyy,comp}
    %\icmlauthor{}{sch}
    %\icmlauthor{}{sch}
  \end{icmlauthorlist}

  \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
  \icmlaffiliation{comp}{Company Name, Location, Country}
  \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

  \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
  \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

  % You may provide any keywords that you find helpful for describing your
  % paper; these are used to populate the "keywords" metadata in the PDF but
  % will not be shown in the document
  \icmlkeywords{Machine Learning, ICML}

  \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column listing the
% affiliations and the copyright notice. The command takes one argument, which
% is text to display at the start of the footnote. The \icmlEqualContribution
% command is standard text for equal contribution. Remove it (just {}) if you
% do not need this facility.

% Use ONE of the following lines. DO NOT remove the command.

% If you have no special notice, KEEP empty braces:
\printAffiliationsAndNotice{}  % no special notice (required even if empty)
% Or, if applicable, use the standard equal contribution text:
% \printAffiliationsAndNotice{\icmlEqualContribution}


\begin{abstract}

We propose \textbf{SPOCK} (Scalable and Structure-Preserving Optimal Transport based Clustering with Kernel-density-estimation), a robust framework for clustering large-scale, incomplete, and unaligned multi-view data. Existing methods often suffer from quadratic or cubic computational complexity or rely on rigid assumptions about data quality. SPOCK addresses these challenges through a unified design philosophy: preserving and enhancing the intrinsic geometric structure of data while maintaining end-to-end near-linear complexity. The framework operates in three phases: 1) Structure-Preserving Sparse Feature Selection, which leverages Randomized SVD with sparse Laplacian regularization to efficiently learn a denoised, informative feature subspace in $O(NDd)$ complexity; 2) Density-Aware View Weighting, which employs scalable $k$-NN based Kernel Density Estimation to compute adaptive view quality weights in $O(Nk\log N)$ time; and 3) OT-Enhanced Spectral Clustering, which utilizes a novel landmark-based Sinkhorn algorithm with $M \ll N$ landmarks to refine graph structure while maintaining $O(NMT)$ complexity. Experiments demonstrate SPOCK achieves 96\% clustering accuracy while maintaining near-linear scalability, outperforming state-of-the-art methods particularly in settings with missing or misaligned views.

\end{abstract}

\section{Introduction}\label{introduction}

Multi-view learning has emerged as a powerful paradigm for analyzing
complex data from multiple sources or modalities. However, real-world
applications often present challenges such as high-dimensional feature
spaces, unaligned views, and incomplete data, which hinder the
effectiveness of traditional methods. While existing approaches like
co-training, multiple kernel learning, and subspace learning have shown
promise, they frequently suffer from scalability issues when dealing
with sparse or misaligned views \cite{XuTaoXu2013}.

Kernel density estimation (KDE) is a widely used non-parametric
technique for estimating probability distributions, but its
computational cost grows quadratically with dimensionality \cite{Wglarczyk2018}, which makes it impractical for high-dimensional
multi-view data, where the number of required kernels becomes
prohibitively large. Compressive sensing offers a potential solution by
exploiting sparsity to approximate signals with fewer measurements
\cite{Baraniuk2007}. Recent work has explored its application in
multi-view learning, yet few methods explicitly address the dual
challenges of scalability and adaptability in the presence of incomplete
or unaligned views.

To settle this problem, we propose a novel approach that integrates sparse feature selection
with compressive KDE to overcome these limitations. Unlike existing
methods such as TUMCR \cite{JiFengLi2024} or VITAL \cite{KimRenChartonHuothers2024}, which focus on adaptive learning or deep
representation fusion, our method explicitly targets the
computational bottleneck of density estimation in high dimensions. By
identifying and retaining only the most informative features, we reduce
the number of kernels required for KDE while preserving discriminative
power. The sparse feature representations are then fused using a
graph-based contrastive learning framework, which aligns views through
optimal transport with entropy regularization. This not only mitigates
the curse of dimensionality but also enhances robustness to missing or
misaligned data.

The main contributions of our work are as follows. First, we introduce a
sparse compressive KDE framework that significantly reduces
computational complexity while maintaining estimation accuracy. Second,
we develop an adaptive kernel bandwidth selection mechanism that
optimizes entropy per view, ensuring robustness to varying data
densities. Third, we integrate these components into a scalable
multi-view learning pipeline.
% that outperforms state-of-the-art methods in both clustering and classification tasks.

The remainder of this paper is organized as follows: Section 2 reviews
related work in multi-view learning, KDE, and compressive sensing.
Section 3 introduces the mathematical preliminaries of multi-view
kernels, KDE, and compressive sensing. Section 4 details our method. Section 5
presents experimental results on benchmark datasets, and Section 6
discusses implications and future directions. Finally, Section 7
concludes the paper.

% Our approach builds on recent advances in sparse representation learning
% and compressive sensing, but extends them to the multi-view setting with
% a focus on scalability and adaptability. Unlike deep learning-based
% methods such as MvCLN or GVMVC, which require extensive training data
% and computational resources \cite{LiangWangNieLi2020},
% \cite{GunionHaber2003}, our framework is lightweight and modular, making
% it suitable for applications where data is scarce or computational
% budgets are limited. By bridging the gap between sparse feature
% selection and density estimation, we provide a principled solution to
% the challenges of high-dimensional, unaligned, and incomplete multi-view
% learning.

\section{Related Work}\label{related-work}

Multi-view learning has evolved through several paradigms. Early approaches focused on co-training and co-regularization \cite{Sun2013}. While effective for aligned views, these methods struggle with high-dimensional or incomplete data.

\subsection{Scalable Feature Selection}
Traditional feature selection often relies on variance maximization (e.g., PCA) or pseudo-labels. However, full SVD incurs $O(\min(N, D) \cdot N \cdot D)$ complexity, which is prohibitive for high-dimensional data. Randomized SVD \cite{halko2011finding} provides a near-optimal low-rank approximation in $O(NDd)$ time by leveraging random projections. Recent works like TGL \cite{11205298} have explored local structure preservation, but often incur $O(N^2)$ costs for dense Laplacian construction. Our approach integrates randomized SVD with \textbf{sparse} Laplacian regularization, reducing complexity to $O(NDd + Nkd)$.

\subsection{Landmark-based Optimal Transport}
Optimal Transport (OT) provides a robust framework for distribution alignment but standard Sinkhorn-Knopp requires $O(N^2)$ per iteration for the full kernel matrix. Recent advances explore landmark-based OT \cite{forrow2019statistical}, which selects $M \ll N$ representative points and solves a reduced $N \times M$ transport problem. SPOCK adopts this strategy with density-weighted landmark selection, achieving $O(NMT)$ complexity while preserving transport quality.

\subsection{Sparse Spectral Clustering}
Graph-based methods \cite{YangLiHuBaiLvothers2022} and deep learning variants \cite{JinWangDongLiuothers2023} have been proposed for multi-view clustering. However, dense spectral clustering requires $O(N^3)$ eigendecomposition. By maintaining sparse $k$-NN graphs with $O(Nk)$ edges, iterative eigensolvers (e.g., ARPACK) achieve $O(Nk \cdot K)$ complexity for $K$ eigenvectors. SPOCK combines sparse graphs with OT-enhanced edge weights to improve cluster quality without sacrificing scalability.

\section{Preliminaries}\label{preliminaries}

\subsection{$k$-NN Based Kernel Density Estimation}
Kernel Density Estimation (KDE) estimates the probability density function $p(\mathbf{x})$ from samples. Standard KDE requires $O(N^2)$ pairwise kernel evaluations:
\[
\hat{p}(\mathbf{x}) = \frac{1}{N} \sum_{i=1}^N K_\sigma(\mathbf{x}, \mathbf{x}_i)
\]
where $K_\sigma$ is typically a Gaussian kernel with bandwidth $\sigma$.

To achieve near-linear complexity, we approximate KDE using only $k$-nearest neighbors:
\[
\hat{p}(\mathbf{x}_i) \approx \frac{1}{k} \sum_{j \in \mathcal{N}_k(i)} K_\sigma(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\frac{\bar{d}_i^2}{2\sigma^2}\right)
\]
where $\bar{d}_i$ is the average distance to the $k$ nearest neighbors. Using Ball Tree for $k$-NN search, this achieves $O(Nk\log N)$ complexity.

\subsection{Landmark-based Optimal Transport}
Optimal Transport finds the most efficient way to transform one distribution to another. The discrete entropy-regularized formulation is:
\[
\min_{\mathbf{T} \geq 0} \langle \mathbf{T}, \mathbf{C} \rangle + \varepsilon H(\mathbf{T}), \quad \text{s.t. } \mathbf{T}\mathbf{1} = \mathbf{a}, \quad \mathbf{T}^\top\mathbf{1} = \mathbf{b}
\]

The Sinkhorn-Knopp algorithm solves this via iterative scaling, but requires $O(N^2)$ for the full $N \times N$ kernel matrix. By introducing $M \ll N$ landmark points, we solve a reduced $N \times M$ transport problem:
\[
\mathbf{T}^* = \text{diag}(\mathbf{u}) \cdot \exp(-\mathbf{C}/\varepsilon) \cdot \text{diag}(\mathbf{v})
\]
where $\mathbf{C} \in \mathbb{R}^{N \times M}$ is the cost matrix and the Sinkhorn iterations cost $O(NM)$ each, totaling $O(NMT)$ for $T$ iterations.

\section{SPOCK: Scalable and Structure-Preserving Optimal Transport based Clustering with Kernel-density-estimation for Imperfect Multi-View Data}\label{spock-methodology}

The SPOCK framework addresses the computational challenges of high-dimensional multi-view learning through a three-phase pipeline: (1) Structure-Preserving Sparse Feature Selection via Randomized SVD, (2) Density-Aware View Weighting via $k$-NN KDE, and (3) OT-Enhanced Spectral Clustering via Landmark Sinkhorn.

\begin{figure}
\centering
\fbox{%
    \parbox{\linewidth}{\centering
        \vspace{2.5cm}
        \textbf{Placeholder: SPOCK workflow diagram}
        \vspace{2.5cm}
    }%
}
% \includegraphics[width=1\textwidth]{1.png}
\caption{Workflow of SPOCK}
\end{figure}

\subsection{Phase 1: Structure-Preserving Sparse Feature Selection}\label{phase-1}

The primary objective of this phase is to learn a robust, low-dimensional feature representation that preserves the intrinsic geometric structure of the data while maintaining near-linear computational complexity. We achieve this through a two-step approach: (1) Randomized SVD for efficient dimensionality reduction, and (2) Sparse Laplacian regularization for structure preservation.

Let $\mathbf{X} \in \mathbb{R}^{N \times D}$ denote the input data matrix for a single view. For high-dimensional data ($D > 500$), we employ Randomized SVD to efficiently compute the projection matrix $\mathbf{P} \in \mathbb{R}^{D \times d}$. The algorithm proceeds as follows:
\begin{enumerate}
    \item Generate a random projection matrix $\mathbf{\Omega} \in \mathbb{R}^{D \times (d + p)}$, where $p$ is a small oversampling parameter.
    \item Compute the sketch $\mathbf{Y} = \mathbf{X}\mathbf{\Omega}$ in $O(ND(d+p))$ time.
    \item Perform QR decomposition: $\mathbf{Y} = \mathbf{Q}\mathbf{R}$, yielding an orthonormal basis $\mathbf{Q}$.
    \item Project $\mathbf{X}$ onto $\mathbf{Q}$: $\mathbf{B} = \mathbf{Q}^\top \mathbf{X}$.
    \item Compute SVD of the small matrix: $\mathbf{B} = \mathbf{U}_B \mathbf{\Sigma} \mathbf{V}^\top$.
    \item The projection matrix is $\mathbf{P} = \mathbf{V}_{:d}^\top$, the top $d$ right singular vectors.
\end{enumerate}

To preserve local manifold structure, we construct a \textbf{sparse} $k$-nearest neighbor graph using Ball Tree, which achieves $O(Nk\log N)$ complexity. The sparse adjacency matrix $\mathbf{A}$ contains only $O(Nk)$ non-zero entries with heat kernel weights:
\[
A_{ij} = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma^2}\right) \cdot \mathbb{I}(j \in \mathcal{N}_k(i))
\]
where $\sigma$ is set to the median neighbor distance and $\mathcal{N}_k(i)$ denotes the $k$-nearest neighbors of point $i$.

The normalized graph Laplacian $\mathbf{L} = \mathbf{I} - \mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ is stored in sparse format. We then refine the projection matrix via a light gradient step:
\[
\mathbf{P} \leftarrow \mathbf{P} - \eta \cdot \mathbf{X}^\top \mathbf{L} (\mathbf{X}\mathbf{P})
\]
followed by re-orthogonalization via QR decomposition. This sparse Laplacian regularization costs only $O(Nkd)$ due to the sparsity of $\mathbf{L}$.

For self-expression, each point $\mathbf{z}_i = \mathbf{x}_i \mathbf{P}$ is reconstructed using only its $k$ neighbors via local sparse coding:
\[
\min_{\mathbf{w}_i} \|\mathbf{z}_i - \mathbf{Z}_{\mathcal{N}_k(i)} \mathbf{w}_i\|_2^2 + \beta \|\mathbf{w}_i\|_1
\]
This is solved using Iterative Soft Thresholding Algorithm (ISTA) in $O(k^3)$ per point, yielding total complexity $O(Nk^3)$. The result is a sparse self-expression matrix $\mathbf{S}$ with $O(Nk)$ non-zero entries.

The final projected features are row-normalized: $\tilde{\mathbf{X}} = \text{normalize}(\mathbf{X}\mathbf{P})$.

\subsection{Phase 2: Density-Aware View Weighting}\label{phase-2}

Building upon the structure-preserving features $\tilde{\mathbf{X}}^{(v)}$ obtained in Phase 1, the second phase computes adaptive quality weights for each view based on local density estimation. Unlike approaches that require expensive pairwise computations, we leverage $k$-NN based density estimation to achieve $O(Nk\log N)$ complexity per view.

For each view $v$, we first perform a $k$-NN search using Ball Tree, which provides $O(N\log N)$ query time. For each point $\mathbf{x}_i$, we estimate its local density using the average distance to its $k$ nearest neighbors:
\[
\hat{p}(\mathbf{x}_i) = \exp\left(-\frac{\bar{d}_i^2}{2\sigma^2}\right), \quad \bar{d}_i = \frac{1}{k}\sum_{j \in \mathcal{N}_k(i)} \|\mathbf{x}_i - \mathbf{x}_j\|
\]
where $\sigma$ is the median of all neighbor distances (adaptive bandwidth).

The view quality is computed based on two complementary criteria:

\textbf{1. Density Uniformity:} Views with more uniform density distributions (lower coefficient of variation) are considered higher quality, as they likely contain less noise:
\[
\text{CV}_v = \frac{\text{std}(\hat{p}^{(v)})}{\text{mean}(\hat{p}^{(v)})}, \quad q_{\text{density}}^{(v)} = \frac{1}{1 + \text{CV}_v}
\]

\textbf{2. Feature Compactness:} Views with smaller average neighbor distances indicate tighter cluster structures:
\[
q_{\text{compact}}^{(v)} = \frac{1}{1 + \bar{d}_{\text{avg}}^{(v)}}
\]

The final view weight combines these criteria:
\[
w_v = \frac{0.7 \cdot q_{\text{density}}^{(v)} + 0.3 \cdot q_{\text{compact}}^{(v)}}{\sum_{u=1}^{V} (0.7 \cdot q_{\text{density}}^{(u)} + 0.3 \cdot q_{\text{compact}}^{(u)})}
\]

These view weights are used in Phase 3 to create a weighted concatenation of projected features:
\[
\mathbf{X}_{\text{concat}} = \left[\sqrt{w_1} \tilde{\mathbf{X}}^{(1)}, \sqrt{w_2} \tilde{\mathbf{X}}^{(2)}, \ldots, \sqrt{w_V} \tilde{\mathbf{X}}^{(V)}\right]
\]

The total complexity of Phase 2 is $O(V \cdot N \cdot k \cdot \log N)$, which is near-linear in the number of samples.

\subsection{Phase 3: OT-Enhanced Spectral Clustering}\label{phase-3}

The final phase performs clustering on the weighted concatenated features from Phase 2. The key innovation is using \textbf{landmark-based Optimal Transport} to enhance the $k$-NN graph structure before spectral embedding. This allows OT to refine cluster boundaries while maintaining near-linear complexity.

\textbf{Step 1: Sparse KNN Graph Construction.}
We first build a sparse $k$-NN graph on the concatenated features $\mathbf{X}_{\text{concat}}$ using Ball Tree ($O(Nk\log N)$). For each edge $(i, j)$, the weight is computed using an adaptive heat kernel:
\[
W_{ij} = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma_{ij}^2}\right), \quad \sigma_{ij} = \frac{\sigma_i + \sigma_j}{2}
\]
where $\sigma_i$ is the distance to the $k$-th nearest neighbor of point $i$. The graph is stored in sparse format with $O(Nk)$ edges.

\textbf{Step 2: Landmark-based Sinkhorn OT.}
To avoid $O(N^2)$ pairwise OT computation, we introduce $M \ll N$ landmark points selected via density-weighted $k$-means++ initialization (preferring high-density cluster cores). The landmark selection uses:
\[
P(\text{select } \mathbf{x}_i) \propto \hat{p}(\mathbf{x}_i)^2 \cdot \min_{l \in \mathcal{L}} \|\mathbf{x}_i - \mathbf{x}_l\|^2
\]

We then solve the entropy-regularized OT problem from all $N$ points to $M$ landmarks:
\[
\min_{\mathbf{T} \geq 0} \langle \mathbf{T}, \mathbf{C} \rangle + \varepsilon H(\mathbf{T}), \quad \text{s.t. } \mathbf{T}\mathbf{1}_M = \frac{1}{N}\mathbf{1}_N, \quad \mathbf{T}^\top\mathbf{1}_N = \frac{1}{M}\mathbf{1}_M
\]
where $\mathbf{C} \in \mathbb{R}^{N \times M}$ is the cost matrix (squared Euclidean distances) and $H(\mathbf{T})$ is the entropy. This is solved via the Sinkhorn algorithm in $O(NMT)$ time for $T$ iterations.

The resulting transport plan $\mathbf{T} \in \mathbb{R}^{N \times M}$ encodes a "soft cluster assignment" where each row represents a point's distribution over landmarks. We normalize each row to obtain the \textbf{transport profile}:
\[
\mathbf{t}_i = \frac{\mathbf{T}_{i,:}}{\|\mathbf{T}_{i,:}\|_2}
\]

\textbf{Step 3: Graph Enhancement.}
For each edge $(i, j)$ in the sparse graph, we compute the OT-based similarity as the cosine similarity of transport profiles:
\[
\text{sim}_{\text{OT}}(i, j) = \mathbf{t}_i^\top \mathbf{t}_j
\]

The edge weight is enhanced using an \textbf{additive OT bonus} combined with density correction:
\[
W'_{ij} = W_{ij} \cdot \underbrace{\left(0.5 + 0.5 \cdot (1 - |\hat{p}_i - \hat{p}_j|)\right)}_{\text{density factor}} + \underbrace{\gamma \cdot \max(0, \text{sim}_{\text{OT}}(i, j) - 0.5)}_{\text{OT bonus}}
\]
where $\gamma = 0.06$ controls the OT contribution. This additive formulation ensures OT only provides positive reinforcement for high-similarity pairs, preserving the base graph structure.

\textbf{Step 4: Sparse Spectral Embedding.}
We compute the normalized Laplacian of the enhanced sparse graph:
\[
\mathbf{L}_{\text{norm}} = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{W}' \mathbf{D}^{-1/2}
\]
Using sparse eigensolvers (e.g., ARPACK), we extract the top $K$ eigenvectors in $O(Nk \cdot K)$ time, yielding the spectral embedding $\mathbf{Z} \in \mathbb{R}^{N \times K}$.

\textbf{Step 5: Final Clustering.}
K-Means is applied to the row-normalized embedding $\mathbf{Z}$ with multiple restarts to obtain the final cluster assignments.

\subsection{Overall Framework}\label{overall-objective}

SPOCK integrates three phases into a coherent pipeline, each designed for near-linear scalability:

\textbf{Phase 1 Objective.} For each view $v$, we learn a projection matrix $\mathbf{P}^{(v)}$ via randomized SVD with sparse Laplacian regularization:
\begin{equation}
\mathbf{P}^{(v)} = \arg\min_{\mathbf{P}} \|\mathbf{X}^{(v)} - \mathbf{X}^{(v)}\mathbf{P}\mathbf{P}^\top\|_F^2 + \alpha \cdot \text{Tr}(\mathbf{P}^\top \mathbf{X}^{(v)\top} \mathbf{L}^{(v)} \mathbf{X}^{(v)} \mathbf{P})
\end{equation}
where $\mathbf{L}^{(v)}$ is a sparse normalized Laplacian with $O(Nk)$ non-zeros.

\textbf{Phase 2 Output.} View quality weights $\{w_v\}_{v=1}^V$ based on density uniformity and compactness.

\textbf{Phase 3 Objective.} Given the enhanced sparse graph $\mathbf{W}'$, spectral clustering solves:
\begin{equation}
\min_{\mathbf{Z}} \text{Tr}(\mathbf{Z}^\top \mathbf{L}_{\text{norm}} \mathbf{Z}), \quad \text{s.t. } \mathbf{Z}^\top \mathbf{Z} = \mathbf{I}
\end{equation}
where the edge weights incorporate both density correction and OT-based similarity.

The key insight is that the sparse graph structure ($O(Nk)$ edges) is preserved throughout, and OT enhancement operates on landmarks ($M \ll N$), ensuring end-to-end near-linear complexity.

\subsection{Optimization Procedure}\label{optimization-procedure}

Each phase of SPOCK is optimized independently with efficient algorithms tailored to maintain near-linear complexity.

\textbf{Phase 1: Randomized SVD + Sparse Laplacian.}
The projection matrix $\mathbf{P}$ is computed in two steps:
\begin{enumerate}
    \item \textbf{Randomized SVD:} For $D > 500$, use random projection with QR decomposition to extract top $d$ singular vectors in $O(NDd)$.
    \item \textbf{Laplacian Refinement:} Apply gradient descent with sparse Laplacian:
    \[
    \mathbf{P} \leftarrow \mathbf{P} - \eta \cdot \mathbf{X}^\top \mathbf{L}_{\text{sparse}} (\mathbf{X}\mathbf{P})
    \]
    followed by QR re-orthogonalization. Cost: $O(Nkd)$.
\end{enumerate}

The sparse self-expression matrix $\mathbf{S}$ is computed via localized ISTA:
\[
\mathbf{w}_i^{(t+1)} = \mathcal{S}_{\beta/L}\left(\mathbf{w}_i^{(t)} - \frac{1}{L}\nabla f(\mathbf{w}_i^{(t)})\right)
\]
where $\mathcal{S}_\tau$ is the soft-thresholding operator and $L$ is the Lipschitz constant. Each point uses only its $k$ neighbors, yielding $O(Nk^3)$ total.

\textbf{Phase 2: KNN-based Density Estimation.}
Using Ball Tree for $k$-NN search achieves $O(Nk\log N)$ per view. Density is estimated from neighbor distances without pairwise kernel computation.

\textbf{Phase 3: Landmark Sinkhorn + Sparse Spectral.}
The Sinkhorn algorithm for $N$ points to $M$ landmarks runs for $T$ iterations:
\begin{align}
\mathbf{v}^{(t+1)} &= \mathbf{b} \oslash (\mathbf{K}^\top \mathbf{u}^{(t)}) \\
\mathbf{u}^{(t+1)} &= \mathbf{a} \oslash (\mathbf{K} \mathbf{v}^{(t+1)})
\end{align}
where $\mathbf{K} = \exp(-\mathbf{C}/\varepsilon) \in \mathbb{R}^{N \times M}$. Each iteration costs $O(NM)$, totaling $O(NMT)$.

Sparse spectral embedding uses ARPACK on the $O(Nk)$-edge graph, costing $O(NkK)$ for $K$ eigenvectors.

The complete procedure is summarized in Algorithm \ref{alg:spock}.

\begin{algorithm}[tb]
   \caption{SPOCK: Scalable Structure-Preserving Clustering}
   \label{alg:spock}
\begin{algorithmic}
   \STATE {\bfseries Input:} Multi-view data $\{\mathbf{X}^{(v)}\}_{v=1}^V$, clusters $K$, neighbors $k$, landmarks $M$.
   \STATE {\bfseries Output:} Cluster assignments $\{y_i\}_{i=1}^N$.
   \STATE
   \STATE \textbf{Phase 1: Structure-Preserving Feature Selection} \hfill $O(V \cdot N \cdot D \cdot d)$
   \FOR{each view $v = 1, \ldots, V$}
   \STATE Build sparse KNN graph using Ball Tree \hfill $O(Nk\log N)$
   \STATE Compute sparse Laplacian $\mathbf{L}^{(v)}$ with $O(Nk)$ edges
   \STATE $\mathbf{P}^{(v)} \leftarrow$ Randomized SVD$(\mathbf{X}^{(v)}, d)$ \hfill $O(NDd)$
   \STATE $\mathbf{P}^{(v)} \leftarrow \mathbf{P}^{(v)} - \eta \mathbf{X}^{(v)\top} \mathbf{L}^{(v)} \mathbf{X}^{(v)} \mathbf{P}^{(v)}$ \hfill $O(Nkd)$
   \STATE $\mathbf{S}^{(v)} \leftarrow$ Local Sparse Coding (ISTA, $k$ neighbors) \hfill $O(Nk^3)$
   \STATE $\tilde{\mathbf{X}}^{(v)} \leftarrow \text{normalize}(\mathbf{X}^{(v)}\mathbf{P}^{(v)})$
   \ENDFOR
   \STATE
   \STATE \textbf{Phase 2: Density-Aware View Weighting} \hfill $O(V \cdot N \cdot k \cdot \log N)$
   \FOR{each view $v = 1, \ldots, V$}
   \STATE $k$-NN search on $\tilde{\mathbf{X}}^{(v)}$ using Ball Tree \hfill $O(Nk\log N)$
   \STATE Estimate density $\hat{p}^{(v)}_i = \exp(-\bar{d}_i^2 / 2\sigma^2)$
   \STATE $q^{(v)} \leftarrow 0.7/(1+\text{CV}) + 0.3/(1+\bar{d}_{\text{avg}})$
   \ENDFOR
   \STATE $w_v \leftarrow q^{(v)} / \sum_u q^{(u)}$ for all $v$
   \STATE $\mathbf{X}_{\text{concat}} \leftarrow [\sqrt{w_1}\tilde{\mathbf{X}}^{(1)}, \ldots, \sqrt{w_V}\tilde{\mathbf{X}}^{(V)}]$
   \STATE
   \STATE \textbf{Phase 3: OT-Enhanced Spectral Clustering} \hfill $O(Nk\log N + NMT)$
   \STATE Build sparse KNN graph $\mathbf{W}$ on $\mathbf{X}_{\text{concat}}$ \hfill $O(Nk\log N)$
   \STATE Select $M$ landmarks via density-weighted $k$-means++
   \STATE Compute cost matrix $\mathbf{C} \in \mathbb{R}^{N \times M}$
   \STATE $\mathbf{T} \leftarrow$ Sinkhorn$(\mathbf{C}, \varepsilon)$ for $T$ iterations \hfill $O(NMT)$
   \STATE Normalize: $\mathbf{t}_i \leftarrow \mathbf{T}_{i,:} / \|\mathbf{T}_{i,:}\|_2$
   \FOR{each edge $(i, j)$ in sparse graph}
   \STATE $\text{boost}_{ij} \leftarrow (0.5 + 0.5|1 - |\hat{p}_i - \hat{p}_j||) + 0.06 \max(0, \mathbf{t}_i^\top \mathbf{t}_j - 0.5)$
   \STATE $W'_{ij} \leftarrow W_{ij} \cdot \text{boost}_{ij}$
   \ENDFOR
   \STATE $\mathbf{Z} \leftarrow$ Top-$K$ eigenvectors of sparse $\mathbf{L}_{\text{norm}}$ \hfill $O(NkK)$
   \STATE $\{y_i\} \leftarrow$ K-Means$(\mathbf{Z}, K)$
\end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}\label{complexity-analysis}

We provide a detailed complexity analysis of SPOCK to demonstrate its scalability. Let $N$ be the number of samples, $D$ the feature dimension, $V$ the number of views, $d$ the projection dimension, $k$ the number of neighbors, $M$ the number of OT landmarks, $T$ the Sinkhorn iterations, and $K$ the number of clusters.

\begin{table}[h]
\centering
\caption{Per-Phase Complexity of SPOCK}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Complexity} & \textbf{Dominant Term} \\
\midrule
\multicolumn{3}{l}{\textit{Phase 1: Feature Selection (per view)}} \\
Sparse KNN Graph & $O(Nk\log N)$ & Ball Tree \\
Randomized SVD & $O(NDd)$ & Random projection \\
Laplacian Refinement & $O(Nkd)$ & Sparse $\mathbf{L} \times \mathbf{Z}$ \\
Local Sparse Coding & $O(Nk^3)$ & ISTA per point \\
\midrule
\multicolumn{3}{l}{\textit{Phase 2: View Weighting (per view)}} \\
$k$-NN Density & $O(Nk\log N)$ & Ball Tree \\
Quality Metrics & $O(N)$ & Statistics \\
\midrule
\multicolumn{3}{l}{\textit{Phase 3: OT-Enhanced Clustering}} \\
Sparse KNN Graph & $O(Nk\log N)$ & Ball Tree \\
Landmark Selection & $O(NM)$ & $k$-means++ \\
Cost Matrix & $O(NMd_{\text{concat}})$ & Distances \\
Sinkhorn OT & $O(NMT)$ & $T$ iterations \\
Graph Enhancement & $O(Nk)$ & Sparse edges \\
Sparse Eigensolver & $O(NkK)$ & ARPACK \\
K-Means & $O(NKI)$ & $I$ iterations \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Total Complexity.} Summing all components:
\[
\mathcal{O}\left(V \cdot N \cdot (D \cdot d + k \cdot \log N + k^3) + N \cdot (M \cdot T + k \cdot K)\right)
\]

With typical parameters ($k \ll N$, $M \ll N$, $d \ll D$), this simplifies to:
\[
\mathcal{O}\left(V \cdot N \cdot \bar{D} \cdot d + V \cdot N \cdot k \cdot \log N\right) \approx \mathcal{O}(N \log N)
\]
where $\bar{D} = \frac{1}{V}\sum_v D^{(v)}$ is the average feature dimension.

\textbf{Space Complexity.} The sparse graph uses $O(Nk)$ storage. The OT transport plan requires $O(NM)$, and spectral embedding uses $O(NK)$. Total: $O(N(k + M + K))$, which is linear in $N$.

\textbf{Comparison.} Traditional methods have:
\begin{itemize}
    \item Standard spectral clustering: $O(N^3)$ eigendecomposition
    \item Full KDE: $O(N^2)$ pairwise distances
    \item Full Sinkhorn OT: $O(N^2 T)$ per iteration
\end{itemize}
SPOCK achieves a speedup factor of $O(N / (k \log N + M))$ for large datasets.

\bibliography{ref}
\bibliographystyle{icml2026}
\end{document}
